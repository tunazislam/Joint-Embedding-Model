{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import emoji #pip install emoji --upgrade\n",
    "# Internal dependencies\n",
    "import word_emoji2vec as we2v\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributed as dist\n",
    "from apex.parallel import DistributedDataParallel as DDP \n",
    "from apex import amp\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1, 2, 3' \n",
    "#....to install apex...\n",
    "#pip install -v --no-cache-dir ./\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed #python -m spacy download en\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.enabled = False\n",
    "np.random.seed(1)\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "# np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load network embedding\n",
    "net_emb = gsm.KeyedVectors.load_word2vec_format('data/userNetworkEmd.emd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/yoga_user_name_loc_des_mergetweets_yoga_1300_lb.csv\") \n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load train users and split into train and validation #######\n",
    "import random\n",
    "with open(\"data/train.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:830] #80% train  \n",
    "#print(train_data, len(train_data)) #830\n",
    "valid_data = data[830:] #20% validation\n",
    "#print(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####.........Create dataframe for Train data\n",
    "name = []\n",
    "location = []\n",
    "description = []\n",
    "text = []\n",
    "utype = []\n",
    "umotivation = []\n",
    "for i in range (0,len(train_data)):\n",
    "    for j in range (0,df.shape[0]):\n",
    "        if df.name[j] == train_data[i]:\n",
    "            name.append(df.name[j])\n",
    "            location.append(df.location[j])\n",
    "            description.append(df.description[j])\n",
    "            text.append(df.text[j])\n",
    "            utype.append(df.utype[j])\n",
    "            umotivation.append(df.umotivation[j])\n",
    "            break\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(data= name, columns=['Name'])\n",
    "df_train['location'] = location\n",
    "df_train['description'] = description   \n",
    "df_train['text'] = text\n",
    "df_train['utype'] = utype  \n",
    "df_train['umotivation'] = umotivation   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####.........Create dataframe for validation data\n",
    "name = []\n",
    "location = []\n",
    "description = []\n",
    "text = []\n",
    "utype = []\n",
    "umotivation = []\n",
    "for i in range (0,len(valid_data)):\n",
    "    for j in range (0,df.shape[0]):\n",
    "        if df.name[j] == valid_data[i]:\n",
    "            name.append(df.name[j])\n",
    "            location.append(df.location[j])\n",
    "            description.append(df.description[j])\n",
    "            text.append(df.text[j])\n",
    "            utype.append(df.utype[j])\n",
    "            umotivation.append(df.umotivation[j])\n",
    "            break\n",
    "\n",
    "\n",
    "df_valid = pd.DataFrame(data= name, columns=['Name'])\n",
    "df_valid['location'] = location\n",
    "df_valid['description'] = description   \n",
    "df_valid['text'] = text\n",
    "df_valid['utype'] = utype  \n",
    "df_valid['umotivation'] = umotivation   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########.....Load Test data.......\n",
    "with open(\"data/test.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "test_data = data[:] \n",
    "#print(test_data, len(test_data)) #262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####.........Create dataframe for Test data\n",
    "name = []\n",
    "location = []\n",
    "description = []\n",
    "text = []\n",
    "utype = []\n",
    "umotivation = []\n",
    "for i in range (0,len(test_data)):\n",
    "    for j in range (0,df.shape[0]):\n",
    "        if df.name[j] == test_data[i]:\n",
    "            name.append(df.name[j])\n",
    "            location.append(df.location[j])\n",
    "            description.append(df.description[j])\n",
    "            text.append(df.text[j])\n",
    "            utype.append(df.utype[j])\n",
    "            umotivation.append(df.umotivation[j])\n",
    "            break\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame(data= name, columns=['Name'])\n",
    "df_test['location'] = location\n",
    "df_test['description'] = description   \n",
    "df_test['text'] = text\n",
    "df_test['utype'] = utype  \n",
    "df_test['umotivation'] = umotivation   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### text preprocessing #########\n",
    "\n",
    "def remove_string_noise(input_str):\n",
    "    input_str = re.sub(r\"http\\S+\", \"\", input_str)\n",
    "    \n",
    "    #give special char you want to remove\n",
    "    #do not put space between chars, and space (\" \") is not a special char\n",
    "    punctuation_noise =\"!\\\"$%&'#()*+,-./:;<=>?@[\\]^_`{|}~\" #print string.punctuation \n",
    "    #number_noise = \"0123456789\"\n",
    "    special_noise = \"\"\n",
    "\n",
    "    #all_noise = punctuation_noise + number_noise + special_noise\n",
    "    all_noise = punctuation_noise + special_noise\n",
    "\n",
    "    for c in all_noise:\n",
    "        if c in input_str:\n",
    "            input_str = input_str.replace(c, \" \")#replace with space\n",
    "    fresh_str = ' '.join(input_str.split())\n",
    "    return fresh_str\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#     sw = set(stopwords.words('english'))\n",
    "#     operators = set(('no', 'not', 'nor', 'none'))\n",
    "#     stop_words = set(sw) - operators\n",
    "#     stop_words.update([ 'amp', 'rt'])  ###as we are using set so we used .update....otherwise .extends\n",
    "    \n",
    "#     tweet = re.sub(r':', '', tweet)\n",
    "#     tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "#     tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    word_tokens = nltk.word_tokenize(tweet)\n",
    "    filtered_tweet = []\n",
    "    for w in word_tokens:\n",
    "        #if w not in stop_words and w not in emoticons:\n",
    "        #if w not in stop_words:\n",
    "        filtered_tweet.append(w)\n",
    "    lemmatized_tweet = []\n",
    "    for word in filtered_tweet:\n",
    "        lemmatized_tweet.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "\n",
    "    #print (sentences)\n",
    "    #return ' '.join(filtered_tweet)\n",
    "    return ' '.join(lemmatized_tweet)\n",
    "    #return ' '.join(sentences)\n",
    "    \n",
    "def pre_processing_tweets (df):\n",
    "    #p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.MENTION, p.OPT.RESERVED)\n",
    "    p.set_options(p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.MENTION, p.OPT.RESERVED)\n",
    "    \n",
    "    clean_text = []\n",
    "    #for i in range (0, df.shape[0]):\n",
    "        #clean_text.append(p.clean(str(df['text'][i])))#python 3\n",
    "\n",
    "    for i in df:\n",
    "        clean_text.append(p.clean(str(i)))#python 3\n",
    "    #print(clean_text) #all ok\n",
    "    fresh_text1 = []\n",
    "    for i in range (0, df.shape[0]):\n",
    "        #fresh_text1.append(remove_string_noise(clean_text[i].encode('ascii', 'ignore').decode(\"utf-8\"))) #can remove other emojis and no \\UF..\n",
    "        fresh_text1.append(remove_string_noise(clean_text[i]))\n",
    "    #print(fresh_text1) #all ok\n",
    "    \n",
    "    #Call clean_tweet method-2 for extra preprocessing\n",
    "    filtered_tweet = []\n",
    "    for i in range (0, len(fresh_text1)):\n",
    "        filtered_tweet.append(clean_tweets(fresh_text1[i].lower()))\n",
    "    return filtered_tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train_loc = pre_processing_tweets(df_train.location)\n",
    "sentences_valid_loc = pre_processing_tweets(df_valid.location)\n",
    "sentences_test_loc = pre_processing_tweets(df_test.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train_des = pre_processing_tweets(df_train.description)\n",
    "sentences_valid_des = pre_processing_tweets(df_valid.description)\n",
    "sentences_test_des = pre_processing_tweets(df_test.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train_twt = pre_processing_tweets(df_train.text)\n",
    "sentences_valid_twt = pre_processing_tweets(df_valid.text)\n",
    "sentences_test_twt = pre_processing_tweets(df_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "\n",
    "sentences_train_loc = np.array(sentences_train_loc)\n",
    "sentences_train_des = np.array(sentences_train_des)\n",
    "sentences_train_twt = np.array(sentences_train_twt)\n",
    "labels_train = torch.tensor(df_train.umotivation.values)\n",
    "#print(type(sentences_train_loc))\n",
    "\n",
    "sentences_valid_loc = np.array(sentences_valid_loc)\n",
    "sentences_valid_des = np.array(sentences_valid_des)\n",
    "sentences_valid_twt = np.array(sentences_valid_twt)\n",
    "labels_valid = torch.tensor(df_valid.umotivation.values)\n",
    "#print(type(sentences_valid_loc))\n",
    "\n",
    "sentences_test_loc = np.array(sentences_test_loc)\n",
    "sentences_test_des = np.array(sentences_test_des)\n",
    "sentences_test_twt = np.array(sentences_test_twt)\n",
    "labels_test = torch.tensor(df_test.umotivation.values)\n",
    "#print(type(sentences_test_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first four features are in tokenizer.encode, but I’m using tokenizer.encode_plus to get the fifth item (attention masks).\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "def token_id(sentences):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = bert_tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 160,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            truncation=True, #explicitely truncate examples to max length. #my add\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    #print(len(input_ids), type(input_ids), input_ids[0], input_ids[0].size() ) #torch.Size([1, 160])\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    #labels = torch.tensor(labels)\n",
    "\n",
    "#     # Print sentence 0, now as a list of IDs.\n",
    "#     print('Original: ', sentences_train[0])\n",
    "#     print('Token IDs:', input_ids_train[0])\n",
    "    return input_ids, attention_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train_loc, attention_masks_train_loc = token_id(sentences_train_loc )\n",
    "input_ids_valid_loc, attention_masks_valid_loc = token_id(sentences_valid_loc )\n",
    "input_ids_test_loc, attention_masks_test_loc = token_id(sentences_test_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train_des, attention_masks_train_des = token_id(sentences_train_des )\n",
    "input_ids_valid_des, attention_masks_valid_des = token_id(sentences_valid_des )\n",
    "input_ids_test_des, attention_masks_test_des = token_id(sentences_test_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerModel, LongformerTokenizer, LongformerConfig\n",
    "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first four features are in tokenizer.encode, but I’m using tokenizer.encode_plus to get the fifth item (attention masks).\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "def token_id_twt(sentences):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = longformer_tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 4000,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            truncation=True, #explicitely truncate examples to max length. #my add\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        #print(type(input_ids[0]), input_ids[0], input_ids[0].size())\n",
    "        #sys.exit()\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    #labels = torch.tensor(labels)\n",
    "\n",
    "#     # Print sentence 0, now as a list of IDs.\n",
    "#     print('Original: ', sentences_train[0])\n",
    "#     print('Token IDs:', input_ids_train[0])\n",
    "    return input_ids, attention_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train_twt, attention_masks_train_twt = token_id_twt(sentences_train_twt)\n",
    "input_ids_valid_twt, attention_masks_valid_twt = token_id_twt(sentences_valid_twt )\n",
    "input_ids_test_twt, attention_masks_test_twt = token_id_twt(sentences_test_twt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train_twt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare user network data for neural net#########\n",
    "def nn_input_network(train_data,df):\n",
    "    training_data =[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                #print(train_data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                #print (\"net_emb[train_data[i]] : \", net_emb[train_data[i]], type(net_emb[train_data[i]]), torch.Tensor(net_emb[train_data[i]]), type(torch.Tensor(net_emb[train_data[i]])))\n",
    "                #count = 0\n",
    "                if(train_data[i] not in net_emb ):\n",
    "                    net_emb[train_data[i]] = np.zeros(300) #For users not appearing in the mention network, we set their network embedding vectors as 0.\n",
    "                    #count = count + 1\n",
    "                #print(count)\n",
    "                #print(net_emb[train_data[i]]) #ok\n",
    "                ####.....convert ndarray to torch.tensor........\n",
    "                net_emb_tensor = torch.Tensor(net_emb[train_data[i]])\n",
    "                #print(net_emb_tensor) #ok\n",
    "                training_data.append(net_emb_tensor.unsqueeze(0))\n",
    "                break\n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare training data for neural net #########\n",
    "training_data_net =  nn_input_network(train_data,df)\n",
    "#####prepare validation data for neural net #########\n",
    "validation_data_net =  nn_input_network(valid_data,df)\n",
    "#####prepare Testing data for neural net #########\n",
    "testing_data_net =  nn_input_network(test_data,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(training_data_net), type(training_data_net), training_data_net[0], training_data_net[0].size() ) #torch.Size([1, 160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the network lists into tensors.\n",
    "training_data_net = torch.cat(training_data_net, dim=0)\n",
    "validation_data_net = torch.cat(validation_data_net, dim=0)\n",
    "testing_data_net = torch.cat(testing_data_net, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(input_ids_train_des, input_ids_train_loc, training_data_net, input_ids_train_twt, labels_train)\n",
    "val_dataset = TensorDataset(input_ids_valid_des, input_ids_valid_loc, validation_data_net, input_ids_valid_twt, labels_valid)\n",
    "test_dataset = TensorDataset(input_ids_test_des, input_ids_test_loc, testing_data_net, input_ids_test_twt, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # torch.cuda.empty_cache() \n",
    "# # If there's a GPU available...\n",
    "# if torch.cuda.is_available():    \n",
    "\n",
    "#     # Tell PyTorch to use the GPU.    \n",
    "#     #device = torch.device(\"cuda\") # select the zeroth GPU with this line: gpu = 0\n",
    "#     #device = torch.cuda.set_device(1)  #wrong provide device = None  \n",
    "#     device = torch.device(3) #(use cuda device 1) for gpu = 1\n",
    "#     print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "#     print('We will use the GPU:', torch.cuda.get_device_name())\n",
    "\n",
    "# # If not...\n",
    "# else:\n",
    "#     print('No GPU available, using the CPU instead.')\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "device = torch.device(\"cpu\") # uncomment for cpu use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print ('Current cuda device ', torch.cuda.current_device())\n",
    "# print ('Current cuda device ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_roberta = LongformerModel.from_pretrained('allenai/longformer-base-4096', gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####....Longformer encoding for Tweets\n",
    "from torchnlp.nn import Attention #pip imstall pytorch-nlp\n",
    "class ROBERTALSTMSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 long_roberta,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.long_roberta = long_roberta\n",
    "        \n",
    "        embedding_dim = long_roberta.config.to_dict()['hidden_size'] #768\n",
    "              \n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_fc = Attention(hidden_dim * 2 if bidirectional else hidden_dim) #attention layer from torchnlp\n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        #self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim)\n",
    "        #self.fc1 = nn.Linear(150, output_dim) \n",
    "        #self.fc1 = nn.Linear(hidden_dim, output_dim) \n",
    "        #self.fc2 = nn.Linear(50, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        #print(\"text\", text.size()) #torch.Size([32, 50])       \n",
    "        with torch.no_grad():\n",
    "            embedded = self.long_roberta(text)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        #print(\"embedded\", type(embedded), embedded.size()) #torch.Size([32, 50, 768])\n",
    "        #embedded = self.dropout(embedded) #add dropout...don't use it...dropout layer after an pre-trained embedding, that the weights are not learnt during training\n",
    "        lstm_out, (hidden, c_n) = self.rnn(embedded) #for lstm\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #print(\"hidden before\", type(hidden), hidden.size()) #torch.Size([4, 32, 256])\n",
    "        if self.rnn.bidirectional: #add dropout\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "#         if self.rnn.bidirectional: #No dropout\n",
    "#             hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "#         else:\n",
    "#             hidden = hidden[-1,:,:]\n",
    "            \n",
    "        #lstm_out = self.dropout(lstm_out) #add dropout\n",
    "    \n",
    "        #hidden = [batch size, hid dim]\n",
    "        #print(\"hidden after\", type(hidden), hidden.size()) #torch.Size([32, 512]) cz of bi-direction = 2 * 256 = 512\n",
    "        #attn_out = self.attention(lstm_out, hidden)\n",
    "#         query = hidden = [batch size, output length, dimensions]\n",
    "#         context = lstm_out = [batch size, query length, dimensions]\n",
    "#        input for attention attention(query, context)\n",
    "#       output for attention tuple with output and weights\n",
    "        attn_out = self.attn_fc(hidden.unsqueeze(1), lstm_out)\n",
    "        #print(type(attn_out[0]), attn_out[0].size(), type(attn_out[1]), attn_out[1].size()) #output = torch.Size([32, 1, 512]), weight = torch.Size([32, 1, 160])\n",
    "        #print(\"attn_out\", attn_out.size())\n",
    "        #output = self.out(hidden)\n",
    "        \n",
    "        #output = self.out(attn_out[0].squeeze(1)) #only tweet\n",
    "        #return output\n",
    "        return attn_out[0].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLSTMSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size'] #768\n",
    "              \n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_fc = Attention(hidden_dim * 2 if bidirectional else hidden_dim) #attention layer from torchnlp\n",
    "        #self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        #self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim)\n",
    "        #self.fc1 = nn.Linear(150, output_dim) \n",
    "        #self.fc1 = nn.Linear(hidden_dim, output_dim) \n",
    "        #self.fc2 = nn.Linear(50, output_dim)\n",
    "    \n",
    "    def attention(self, rnn_out, state):\n",
    "        #print(\"rnn_out\", rnn_out.size())        \n",
    "        #print(\"state\", state.size()) \n",
    "        state = state.unsqueeze(2)\n",
    "        #print(\"state\", state.size())\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, state)\n",
    "        #print(\"weights\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        #weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        weights = F.log_softmax(weights.squeeze(2),dim = 1).unsqueeze(2)\n",
    "         #F.log_softmax(x, dim = 1)\n",
    "        #print(\"weights2 :\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    # end method attention\n",
    "\n",
    "    \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        #print(\"text\", text.size()) #torch.Size([32, 50])       \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        #print(\"embedded\", type(embedded), embedded.size()) #torch.Size([32, 50, 768])\n",
    "        #embedded = self.dropout(embedded) #add dropout...don't use it...dropout layer after an pre-trained embedding, that the weights are not learnt during training\n",
    "        lstm_out, (hidden, c_n) = self.rnn(embedded) #for lstm\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #print(\"hidden before\", type(hidden), hidden.size()) #torch.Size([4, 32, 256])\n",
    "        if self.rnn.bidirectional: #add dropout\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "#         query = hidden = [batch size, output length, dimensions]\n",
    "#         context = lstm_out = [batch size, query length, dimensions]\n",
    "#         input for attention attention(query, context)\n",
    "#         output for attention tuple with output and weights\n",
    "\n",
    "        #attn_out = self.attn_fc(hidden.unsqueeze(1), lstm_out)\n",
    "        #print(type(attn_out[0]), attn_out[0].size(), type(attn_out[1]), attn_out[1].size()) #output = torch.Size([32, 1, 512]), weight = torch.Size([32, 1, 160])\n",
    "        #print(\"attn_out\", attn_out[0].size())\n",
    "        #output = self.out(hidden)\n",
    "        \n",
    "        #output = self.out(attn_out[0].squeeze(1)) #original\n",
    "        #output = F.relu(output)\n",
    "        #print(\"output\", type(output), output.size())\n",
    "        \n",
    "        #output = self.fc1(output)\n",
    "        #output = self.fc2(output)\n",
    "        #output = [batch size, out dim]\n",
    "        #print(\"output\", type(output), output.size()) #torch.Size([32, 3])\n",
    "        #return output ##original\n",
    "        #print(hidden.size()) #torch.Size([32, 600])\n",
    "        return hidden\n",
    "        #return attn_out[0].squeeze(1)\n",
    "        #attn_out = self.attention(lstm_out, hidden)\n",
    "        #return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create Model \n",
    "class NetworkMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkMLP, self).__init__() \n",
    "        self.fc1 = nn.Linear(300, 150)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([32, 300])\n",
    "        z1 = self.fc1(X)\n",
    "        #print('z1', z1, z1.size()) # torch.Size([32, 150])\n",
    "        #h1 = F.relu(z1) \n",
    "        return z1 \n",
    "        #return h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointModel(nn.Module):\n",
    "    #def __init__(self):\n",
    "    def __init__(self,\n",
    "                 bert,long_roberta,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "    \n",
    "        #super(JointModel, self).__init__()\n",
    "        super().__init__()\n",
    "        self.model_des = BERTLSTMSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "        self.model_loc = BERTLSTMSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "        self.model_net = NetworkMLP()\n",
    "        self.model_twt = ROBERTALSTMSentiment(long_roberta,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "        self.fc1 = nn.Linear((hidden_dim * 6 +150) if bidirectional else (hidden_dim * 3 +150), 600)\n",
    "        self.fc2 = nn.Linear(600, output_dim) #original\n",
    "#         self.fc2 = nn.Linear(900, 200) #200\n",
    "#         self.fc3 = nn.Linear(200, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x_d, x_l, x_n, x_t): \n",
    "        prediction_des = self.model_des(x_d)\n",
    "        #print(prediction_des, prediction_des.size()) \n",
    "        prediction_loc = self.model_loc(x_l)\n",
    "        #print(prediction_loc, prediction_loc.size()) \n",
    "        prediction_net = self.model_net(x_n)\n",
    "        #print(prediction_net, prediction_net.size()) \n",
    "        prediction_twt = self.model_twt(x_t)\n",
    "        concat_pred = torch.cat((prediction_des, prediction_loc, prediction_net, prediction_twt), 1) #concat with dim= 1\n",
    "        #print(concat_pred, concat_pred.size()) #torch.Size([1, 6])\n",
    "        output = self.fc1(concat_pred)\n",
    "        output = self.dropout(output) # add dropout\n",
    "        output = self.fc2(F.relu(output)) #original\n",
    "#         output = self.dropout(output)#add dropout\n",
    "#         output = self.fc3(F.relu(output))\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN_DIM = 256\n",
    "# OUTPUT_DIM = 3\n",
    "# N_LAYERS = 2\n",
    "# BIDIRECTIONAL = True\n",
    "# DROPOUT = 0.25\n",
    "\n",
    "#HIDDEN_DIM = 256 #original\n",
    "HIDDEN_DIM = 300 #good\n",
    "OUTPUT_DIM = 3\n",
    "N_LAYERS = 2 #original good\n",
    "#N_LAYERS = 3\n",
    "BIDIRECTIONAL = True\n",
    "#DROPOUT = 0.25 #original\n",
    "DROPOUT = 0.5 #good\n",
    "\n",
    "model = JointModel(bert, long_roberta, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL,DROPOUT)\n",
    "\n",
    "# model = BERTLSTMSentiment(bert,\n",
    "#                          HIDDEN_DIM,\n",
    "#                          OUTPUT_DIM,\n",
    "#                          N_LAYERS,\n",
    "#                          BIDIRECTIONAL,\n",
    "#                          DROPOUT)\n",
    "\n",
    "# model = ROBERTALSTMSentiment(long_roberta,\n",
    "#                          HIDDEN_DIM,\n",
    "#                          OUTPUT_DIM,\n",
    "#                          N_LAYERS,\n",
    "#                          BIDIRECTIONAL,\n",
    "#                          DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###In order to freeze paramers (not train them) we need to set their requires_grad attribute to False. To do this, we simply loop through all of the named_parameters in our model and if they're a part of the bert transformer model, we set requires_grad = False.\n",
    "\n",
    "for name, param in model.named_parameters():    \n",
    "    #print(name)\n",
    "    if name.startswith('bert') or name.startswith('long_roberta'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device) \n",
    "criterion = criterion.to(device) \n",
    "\n",
    "# # If we use DataParallel\n",
    "# #  It distributes training into multiple GPUs in a single machine\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "# model.to(device)\n",
    "# criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    #rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    #rounded_preds = torch.round(preds)\n",
    "    correct = (preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    macro_f1 = f1_score(y.to(\"cpu\"), preds.to(\"cpu\"), average='macro')\n",
    "    #print(\"macro_f1\", macro_f1)\n",
    "\n",
    "    return acc, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(iterator):    \n",
    "    #for batch in iterator:\n",
    "        #print(\"batch\", batch)\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "#         b_labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        des = batch[0].to(device)\n",
    "        loc = batch[1].to(device)\n",
    "        net = batch[2].to(device)\n",
    "        twt = batch[3].to(device)\n",
    "        label = batch[4].to(device)\n",
    "        #print(\"label\", label, type(label),label.size()) #torch.Size([32])\n",
    "        #label = label.unsqueeze(1)\n",
    "        #print(\"label\", label, type(label),label.size())\n",
    "        #predictions = model(batch.text).squeeze(1)\n",
    "        #predictions = model(text)\n",
    "        predictions = model(des, loc, net, twt)\n",
    "        #pred = model(text)\n",
    "        #print(\"pred\", pred.size()) #torch.Size([32, 3])\n",
    "        #predictions = torch.argmax(pred, dim = 1)\n",
    "        #print(\"predictions\", predictions, predictions.size()) #torch.Size([32, 3])\n",
    "        #loss = criterion(predictions, batch.label)\n",
    "        loss = criterion(predictions, label)\n",
    "        #print(\"loss\", loss)\n",
    "        #acc = binary_accuracy(predictions, batch.label)\n",
    "        acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "        #print(\"acc\", acc)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #for batch in iterator:\n",
    "        for step, batch in enumerate(iterator):    \n",
    "            des = batch[0].to(device)\n",
    "            loc = batch[1].to(device)\n",
    "            net = batch[2].to(device)\n",
    "            twt = batch[3].to(device)\n",
    "            label = batch[4].to(device)\n",
    "\n",
    "            #predictions = model(batch.text).squeeze(1)\n",
    "            predictions = model(des, loc, net, twt)\n",
    "            #loss = criterion(predictions, batch.label)\n",
    "            loss = criterion(predictions, label)\n",
    "            #acc = binary_accuracy(predictions, batch.label)\n",
    "            acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "per_epoch_train_loss = []\n",
    "per_epoch_val_loss = []\n",
    "per_epoch_train_f1 = []\n",
    "per_epoch_val_f1 = []\n",
    "per_epoch_train_acc = []\n",
    "per_epoch_val_acc = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train(model, train_dataloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(model, validation_dataloader, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    per_epoch_train_loss.append(train_loss)\n",
    "    per_epoch_val_loss.append(valid_loss)\n",
    "    per_epoch_train_f1.append(train_f1)\n",
    "    per_epoch_val_f1.append(valid_f1)\n",
    "    per_epoch_train_acc.append(train_acc)\n",
    "    per_epoch_val_acc.append(valid_acc)\n",
    "    \n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print(\"best model saved in epoch :\", epoch+1 )\n",
    "    torch.save(model.state_dict(), 'data/bert_des+loc+net+twt/dlnt_umotivation'+str(epoch+1)+'.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train macro-avg-f1: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} |  Val. macro-avg-f1: {valid_f1*100:.2f}%')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(train_losses, val_losses, train_f1, val_f1, train_accs, val_accs):\n",
    "    \"\"\"Plot\n",
    "\n",
    "        Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_losses)\n",
    "    xs = np.arange(1,n+1,1)\n",
    "\n",
    "    # plot train and val losses\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "    ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "    #ax.set_xlim(0, 10)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.savefig('data/bert_plot/loss_Bert_DLNT_umotivation.png')\n",
    "\n",
    "    # plot train and val f1-score\n",
    "    #plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_f1, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, val_f1, '-', linewidth=2, label='validation')\n",
    "    \n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Macro-avg F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('data/bert_plot/f1_Bert_DLNT_umotivation.png')\n",
    "    \n",
    "    # plot train and val accuracy\n",
    "    #plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, val_accs, '-', linewidth=2, label='validation')\n",
    "    #ax.set_xlim(0, 10)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('data/bert_plot/acc_Bert_DLNT_umotivation.png')\n",
    "save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, per_epoch_val_f1, per_epoch_train_acc, per_epoch_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Test the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('data/bert_des+loc+net/dln_model_utype1.pt'))\n",
    "\n",
    "# test_loss, test_acc, test_f1 = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "# print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f} | Test macro-avg-f1: {test_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch 3: Test Loss: 0.595 | Test Acc: 77.55 | Test macro-avg-f1: 68.37%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0,10):\n",
    "#     model = JointModel(bert, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL,DROPOUT)\n",
    "#     model = model.to(device)\n",
    "    model.load_state_dict(torch.load('data/bert_des+loc+net+twt/dlnt_umotivation'+str(epoch+1)+'.pt')) \n",
    "    test_loss, test_acc, test_f1 = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f} | Test macro-avg-f1: {test_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
