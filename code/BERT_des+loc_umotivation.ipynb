{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import emoji #pip install emoji --upgrade\n",
    "# Internal dependencies\n",
    "import word_emoji2vec as we2v\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "####BERT\n",
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# # OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "# import logging\n",
    "# #logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# # Load pre-trained model tokenizer (vocabulary)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed #python -m spacy download en\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.enabled = False\n",
    "np.random.seed(1)\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "# np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/yoga_user_name_loc_des_mergetweets_yoga_1300_lb.csv\") \n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load train users and split into train and validation #######\n",
    "import random\n",
    "with open(\"data/train.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:830] #80% train  \n",
    "#print(train_data, len(train_data)) #830\n",
    "valid_data = data[830:] #20% validation\n",
    "#print(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####.........Create dataframe for Train data\n",
    "name = []\n",
    "location = []\n",
    "description = []\n",
    "text = []\n",
    "utype = []\n",
    "umotivation = []\n",
    "for i in range (0,len(train_data)):\n",
    "    for j in range (0,df.shape[0]):\n",
    "        if df.name[j] == train_data[i]:\n",
    "            name.append(df.name[j])\n",
    "            location.append(df.location[j])\n",
    "            description.append(df.description[j])\n",
    "            text.append(df.text[j])\n",
    "            utype.append(df.utype[j])\n",
    "            umotivation.append(df.umotivation[j])\n",
    "            break\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(data= name, columns=['Name'])\n",
    "df_train['location'] = location\n",
    "df_train['description'] = description   \n",
    "df_train['text'] = text\n",
    "df_train['utype'] = utype  \n",
    "df_train['umotivation'] = umotivation   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####.........Create dataframe for validation data\n",
    "name = []\n",
    "location = []\n",
    "description = []\n",
    "text = []\n",
    "utype = []\n",
    "umotivation = []\n",
    "for i in range (0,len(valid_data)):\n",
    "    for j in range (0,df.shape[0]):\n",
    "        if df.name[j] == valid_data[i]:\n",
    "            name.append(df.name[j])\n",
    "            location.append(df.location[j])\n",
    "            description.append(df.description[j])\n",
    "            text.append(df.text[j])\n",
    "            utype.append(df.utype[j])\n",
    "            umotivation.append(df.umotivation[j])\n",
    "            break\n",
    "\n",
    "\n",
    "df_valid = pd.DataFrame(data= name, columns=['Name'])\n",
    "df_valid['location'] = location\n",
    "df_valid['description'] = description   \n",
    "df_valid['text'] = text\n",
    "df_valid['utype'] = utype  \n",
    "df_valid['umotivation'] = umotivation   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########.....Load Test data.......\n",
    "with open(\"data/test.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "test_data = data[:] \n",
    "#print(test_data, len(test_data)) #262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####.........Create dataframe for Test data\n",
    "name = []\n",
    "location = []\n",
    "description = []\n",
    "text = []\n",
    "utype = []\n",
    "umotivation = []\n",
    "for i in range (0,len(test_data)):\n",
    "    for j in range (0,df.shape[0]):\n",
    "        if df.name[j] == test_data[i]:\n",
    "            name.append(df.name[j])\n",
    "            location.append(df.location[j])\n",
    "            description.append(df.description[j])\n",
    "            text.append(df.text[j])\n",
    "            utype.append(df.utype[j])\n",
    "            umotivation.append(df.umotivation[j])\n",
    "            break\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame(data= name, columns=['Name'])\n",
    "df_test['location'] = location\n",
    "df_test['description'] = description   \n",
    "df_test['text'] = text\n",
    "df_test['utype'] = utype  \n",
    "df_test['umotivation'] = umotivation   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### text preprocessing #########\n",
    "\n",
    "def remove_string_noise(input_str):\n",
    "    input_str = re.sub(r\"http\\S+\", \"\", input_str)\n",
    "    \n",
    "    #give special char you want to remove\n",
    "    #do not put space between chars, and space (\" \") is not a special char\n",
    "    punctuation_noise =\"!\\\"$%&'#()*+,-./:;<=>?@[\\]^_`{|}~\" #print string.punctuation \n",
    "    #number_noise = \"0123456789\"\n",
    "    special_noise = \"\"\n",
    "\n",
    "    #all_noise = punctuation_noise + number_noise + special_noise\n",
    "    all_noise = punctuation_noise + special_noise\n",
    "\n",
    "    for c in all_noise:\n",
    "        if c in input_str:\n",
    "            input_str = input_str.replace(c, \" \")#replace with space\n",
    "    fresh_str = ' '.join(input_str.split())\n",
    "    return fresh_str\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#     sw = set(stopwords.words('english'))\n",
    "#     operators = set(('no', 'not', 'nor', 'none'))\n",
    "#     stop_words = set(sw) - operators\n",
    "#     stop_words.update([ 'amp', 'rt'])  ###as we are using set so we used .update....otherwise .extends\n",
    "    \n",
    "#     tweet = re.sub(r':', '', tweet)\n",
    "#     tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "#     tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    word_tokens = nltk.word_tokenize(tweet)\n",
    "    filtered_tweet = []\n",
    "    for w in word_tokens:\n",
    "        #if w not in stop_words and w not in emoticons:\n",
    "        #if w not in stop_words:\n",
    "        filtered_tweet.append(w)\n",
    "    lemmatized_tweet = []\n",
    "    for word in filtered_tweet:\n",
    "        lemmatized_tweet.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "\n",
    "    #print (sentences)\n",
    "    #return ' '.join(filtered_tweet)\n",
    "    return ' '.join(lemmatized_tweet)\n",
    "    #return ' '.join(sentences)\n",
    "    \n",
    "def pre_processing_tweets (df):\n",
    "    #p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.MENTION, p.OPT.RESERVED)\n",
    "    p.set_options(p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.MENTION, p.OPT.RESERVED)\n",
    "    \n",
    "    clean_text = []\n",
    "    #for i in range (0, df.shape[0]):\n",
    "        #clean_text.append(p.clean(str(df['text'][i])))#python 3\n",
    "\n",
    "    for i in df:\n",
    "        clean_text.append(p.clean(str(i)))#python 3\n",
    "    #print(clean_text) #all ok\n",
    "    fresh_text1 = []\n",
    "    for i in range (0, df.shape[0]):\n",
    "        #fresh_text1.append(remove_string_noise(clean_text[i].encode('ascii', 'ignore').decode(\"utf-8\"))) #can remove other emojis and no \\UF..\n",
    "        fresh_text1.append(remove_string_noise(clean_text[i]))\n",
    "    #print(fresh_text1) #all ok\n",
    "    \n",
    "    #Call clean_tweet method-2 for extra preprocessing\n",
    "    filtered_tweet = []\n",
    "    for i in range (0, len(fresh_text1)):\n",
    "        filtered_tweet.append(clean_tweets(fresh_text1[i].lower()))\n",
    "    return filtered_tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train_loc = pre_processing_tweets(df_train.location)\n",
    "sentences_valid_loc = pre_processing_tweets(df_valid.location)\n",
    "sentences_test_loc = pre_processing_tweets(df_test.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train_des = pre_processing_tweets(df_train.description)\n",
    "sentences_valid_des = pre_processing_tweets(df_valid.description)\n",
    "sentences_test_des = pre_processing_tweets(df_test.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df_train.location.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sentences_train_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "\n",
    "sentences_train_loc = np.array(sentences_train_loc)\n",
    "sentences_train_des = np.array(sentences_train_des)\n",
    "# sentences_train_loc = np.array(df_train.location.values)\n",
    "# sentences_train_des = np.array(df_train.description.values)\n",
    "labels_train = torch.tensor(df_train.umotivation.values)\n",
    "#print(type(sentences_train_loc))\n",
    "\n",
    "sentences_valid_loc = np.array(sentences_valid_loc)\n",
    "sentences_valid_des = np.array(sentences_valid_des)\n",
    "# sentences_valid_loc = np.array(df_valid.location.values)\n",
    "# sentences_valid_des = np.array(df_valid.description.values)\n",
    "labels_valid = torch.tensor(df_valid.umotivation.values)\n",
    "#print(type(sentences_valid_loc))\n",
    "\n",
    "sentences_test_loc = np.array(sentences_test_loc)\n",
    "sentences_test_des = np.array(sentences_test_des)\n",
    "# sentences_test_loc = np.array(df_test.location.values)\n",
    "# sentences_test_des = np.array(df_test.description.values)\n",
    "labels_test = torch.tensor(df_test.umotivation.values)\n",
    "#print(type(sentences_test_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first four features are in tokenizer.encode, but I’m using tokenizer.encode_plus to get the fifth item (attention masks).\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "def token_id(sentences):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 160,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            truncation=True, #explicitely truncate examples to max length. #my add\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    #labels = torch.tensor(labels)\n",
    "\n",
    "#     # Print sentence 0, now as a list of IDs.\n",
    "#     print('Original: ', sentences_train[0])\n",
    "#     print('Token IDs:', input_ids_train[0])\n",
    "    return input_ids, attention_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train_loc, attention_masks_train_loc = token_id(sentences_train_loc )\n",
    "input_ids_valid_loc, attention_masks_valid_loc = token_id(sentences_valid_loc )\n",
    "input_ids_test_loc, attention_masks_test_loc = token_id(sentences_test_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train_des, attention_masks_train_des = token_id(sentences_train_des )\n",
    "input_ids_valid_des, attention_masks_valid_des = token_id(sentences_valid_des )\n",
    "input_ids_test_des, attention_masks_test_des = token_id(sentences_test_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(input_ids_train_des, input_ids_train_loc, labels_train)\n",
    "val_dataset = TensorDataset(input_ids_valid_des, input_ids_valid_loc, labels_valid)\n",
    "test_dataset = TensorDataset(input_ids_test_des, input_ids_test_loc, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.cuda.empty_cache() \n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    #device = torch.device(\"cuda\") # select the zeroth GPU with this line: gpu = 0\n",
    "    #device = torch.cuda.set_device(1)  #wrong provide device = None  \n",
    "    device = torch.device(2) #(use cuda device 1) for gpu = 1\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name())\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "#device = torch.device(\"cpu\") # uncomment for cpu use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print ('Current cuda device ', torch.cuda.current_device())\n",
    "print ('Current cuda device ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.nn import Attention #pip imstall pytorch-nlp\n",
    "class BERTLSTMSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size'] #768\n",
    "              \n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_fc = Attention(hidden_dim * 2 if bidirectional else hidden_dim) #attention layer from torchnlp\n",
    "        #self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        #self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim)\n",
    "        #self.fc1 = nn.Linear(150, output_dim) \n",
    "        #self.fc1 = nn.Linear(hidden_dim, output_dim) \n",
    "        #self.fc2 = nn.Linear(50, output_dim)\n",
    "    \n",
    "    def attention(self, rnn_out, state):\n",
    "        #print(\"rnn_out\", rnn_out.size())        \n",
    "        #print(\"state\", state.size()) \n",
    "        state = state.unsqueeze(2)\n",
    "        #print(\"state\", state.size())\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, state)\n",
    "        #print(\"weights\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        #weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        weights = F.log_softmax(weights.squeeze(2),dim = 1).unsqueeze(2)\n",
    "         #F.log_softmax(x, dim = 1)\n",
    "        #print(\"weights2 :\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    # end method attention\n",
    "\n",
    "    \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        #print(\"text\", text.size()) #torch.Size([32, 50])       \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        #print(\"embedded\", type(embedded), embedded.size()) #torch.Size([32, 50, 768])\n",
    "        #embedded = self.dropout(embedded) #add dropout...don't use it...dropout layer after an pre-trained embedding, that the weights are not learnt during training\n",
    "        lstm_out, (hidden, c_n) = self.rnn(embedded) #for lstm\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #print(\"hidden before\", type(hidden), hidden.size()) #torch.Size([4, 32, 256])\n",
    "        if self.rnn.bidirectional: #add dropout\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        return hidden\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointModel(nn.Module):\n",
    "    #def __init__(self):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "    \n",
    "        #super(JointModel, self).__init__()\n",
    "        super().__init__()\n",
    "        self.model_des = BERTLSTMSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "        self.model_loc = BERTLSTMSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "        #self.out = nn.Linear(hidden_dim * 4 if bidirectional else hidden_dim * 2, output_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 4 if bidirectional else hidden_dim * 2, 600)\n",
    "        self.fc2 = nn.Linear(600, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x_d, x_l): \n",
    "        prediction_des = self.model_des(x_d)\n",
    "        #print(prediction_des, prediction_des.size()) #torch.Size([1, 3])\n",
    "        prediction_loc = self.model_loc(x_l)\n",
    "        #print(prediction_loc, prediction_loc.size()) #torch.Size([1, 3])\n",
    "        #prediction_net = self.model_net(x_n)\n",
    "        #print(prediction_net, prediction_net.size()) #torch.Size([1, 3])\n",
    "        #concat_pred = torch.cat((prediction_des, prediction_loc, prediction_net), 1) #concat with dim= 1\n",
    "        concat_pred = torch.cat((prediction_des, prediction_loc), 1) #concat with dim= 1\n",
    "        output = self.fc1(concat_pred)\n",
    "        output = self.dropout(output) # add dropout\n",
    "        output = self.fc2(F.relu(output)) #original\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN_DIM = 256\n",
    "# OUTPUT_DIM = 3\n",
    "# N_LAYERS = 2\n",
    "# BIDIRECTIONAL = True\n",
    "# DROPOUT = 0.25\n",
    "\n",
    "#HIDDEN_DIM = 256 #original\n",
    "HIDDEN_DIM = 300 #good\n",
    "OUTPUT_DIM = 3\n",
    "N_LAYERS = 2 #original good\n",
    "#N_LAYERS = 3\n",
    "BIDIRECTIONAL = True\n",
    "#DROPOUT = 0.25 #original\n",
    "DROPOUT = 0.5 #good\n",
    "\n",
    "model = JointModel(bert, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL,DROPOUT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###In order to freeze paramers (not train them) we need to set their requires_grad attribute to False. To do this, we simply loop through all of the named_parameters in our model and if they're a part of the bert transformer model, we set requires_grad = False.\n",
    "\n",
    "for name, param in model.named_parameters():    \n",
    "    #print(name)\n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():                \n",
    "#     if param.requires_grad:\n",
    "#         print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    #rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    #rounded_preds = torch.round(preds)\n",
    "    correct = (preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    macro_f1 = f1_score(y.to(\"cpu\"), preds.to(\"cpu\"), average='macro')\n",
    "    #print(\"macro_f1\", macro_f1)\n",
    "\n",
    "    return acc, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(iterator):    \n",
    "    #for batch in iterator:\n",
    "        #print(\"batch\", batch)\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "#         b_labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        des = batch[0].to(device)\n",
    "        loc = batch[1].to(device)\n",
    "        label = batch[2].to(device)\n",
    "        #print(\"label\", label, type(label),label.size()) #torch.Size([32])\n",
    "        #label = label.unsqueeze(1)\n",
    "        #print(\"label\", label, type(label),label.size())\n",
    "        #predictions = model(batch.text).squeeze(1)\n",
    "        #predictions = model(text)\n",
    "        predictions = model(des, loc)\n",
    "        #pred = model(text)\n",
    "        #print(\"pred\", pred.size()) #torch.Size([32, 3])\n",
    "        #predictions = torch.argmax(pred, dim = 1)\n",
    "        #print(\"predictions\", predictions, predictions.size()) #torch.Size([32, 3])\n",
    "        #loss = criterion(predictions, batch.label)\n",
    "        loss = criterion(predictions, label)\n",
    "        #print(\"loss\", loss)\n",
    "        #acc = binary_accuracy(predictions, batch.label)\n",
    "        acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "        #print(\"acc\", acc)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #for batch in iterator:\n",
    "        for step, batch in enumerate(iterator):    \n",
    "            des = batch[0].to(device)\n",
    "            loc = batch[1].to(device)\n",
    "            label = batch[2].to(device)\n",
    "\n",
    "            #predictions = model(batch.text).squeeze(1)\n",
    "            predictions = model(des, loc)\n",
    "            #loss = criterion(predictions, batch.label)\n",
    "            loss = criterion(predictions, label)\n",
    "            #acc = binary_accuracy(predictions, batch.label)\n",
    "            acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "per_epoch_train_loss = []\n",
    "per_epoch_val_loss = []\n",
    "per_epoch_train_f1 = []\n",
    "per_epoch_val_f1 = []\n",
    "per_epoch_train_acc = []\n",
    "per_epoch_val_acc = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train(model, train_dataloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(model, validation_dataloader, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    per_epoch_train_loss.append(train_loss)\n",
    "    per_epoch_val_loss.append(valid_loss)\n",
    "    per_epoch_train_f1.append(train_f1)\n",
    "    per_epoch_val_f1.append(valid_f1)\n",
    "    per_epoch_train_acc.append(train_acc)\n",
    "    per_epoch_val_acc.append(valid_acc)\n",
    "    \n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print(\"best model saved in epoch :\", epoch+1 )\n",
    "    torch.save(model.state_dict(), 'data/bert_des+loc/dl_umotivation'+str(epoch+1)+'.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train macro-avg-f1: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} |  Val. macro-avg-f1: {valid_f1*100:.2f}%')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(train_losses, val_losses, train_f1, val_f1, train_accs, val_accs):\n",
    "    \"\"\"Plot\n",
    "\n",
    "        Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_losses)\n",
    "    xs = np.arange(1,n+1,1)\n",
    "\n",
    "    # plot train and val losses\n",
    "    #plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "    ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "    #ax.set_xlim(0, 10)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.savefig('data/bert_plot/loss_Bert_DL_umotivation.png')\n",
    "\n",
    "    # plot train and val f1-score\n",
    "    #plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_f1, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, val_f1, '-', linewidth=2, label='validation')\n",
    "    \n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Macro-avg F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('data/bert_plot/f1_Bert_DL_umotivation.png')\n",
    "    \n",
    "    # plot train and val accuracy\n",
    "    #plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, val_accs, '-', linewidth=2, label='validation')\n",
    "    #ax.set_xlim(0, 10)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('data/bert_plot/acc_Bert_DL_umotivation.png')\n",
    "    \n",
    "save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, per_epoch_val_f1, per_epoch_train_acc, per_epoch_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Test the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0,10):\n",
    "#     model = JointModel(bert, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL,DROPOUT)\n",
    "#     model = model.to(device)\n",
    "    model.load_state_dict(torch.load('data/bert_des+loc/dl_umotivation'+str(epoch+1)+'.pt')) \n",
    "    test_loss, test_acc, test_f1 = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f} | Test macro-avg-f1: {test_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
